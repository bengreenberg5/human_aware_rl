wandb_version: 1

CELL_SIZE:
  desc: null
  value: 256
D2RL:
  desc: null
  value: false
NUM_CONV_LAYERS:
  desc: null
  value: 3
NUM_FILTERS:
  desc: null
  value: 25
NUM_HIDDEN_LAYERS:
  desc: null
  value: 3
SIZE_HIDDEN_LAYERS:
  desc: null
  value: 64
_wandb:
  desc: null
  value:
    cli_version: 0.11.0
    framework: tensorflow
    is_jupyter_run: false
    is_kaggle_kernel: false
    python_version: 3.7.10
    t:
      1:
      - 3
      2:
      - 3
      4: 3.7.10
      5: 0.11.0
      8:
      - 5
bc_model_dir:
  desc: null
  value: /home/anchorwatt/human_aware_rl/human_aware_rl/ppo/bc_runs/default
bc_params:
  desc: null
  value:
    bc_config:
      eager: false
      model_dir: /home/anchorwatt/human_aware_rl/human_aware_rl/ppo/bc_runs/default
      stochastic: true
    bc_policy_cls: human_aware_rl.imitation.behavior_cloning_tf2.BehaviorCloningPolicy
bc_schedule:
  desc: null
  value:
  - - 0
    - 0
  - - .inf
    - 0
bc_stochastic:
  desc: null
  value: true
clip_param:
  desc: null
  value: 0.05
eager:
  desc: null
  value: false
entropy_coeff_end:
  desc: null
  value: 0.1
entropy_coeff_horizon:
  desc: null
  value: 300000.0
entropy_coeff_start:
  desc: null
  value: 0.2
environment_params:
  desc: null
  value:
    env_params:
      horizon: 400
    mdp_params:
      layout_name: cramped_room
      rew_shaping_params:
        DISH_DISP_DISTANCE_REW: 0
        DISH_PICKUP_REWARD: 3
        PLACEMENT_IN_POT_REW: 3
        POT_DISTANCE_REW: 0
        SOUP_DISTANCE_REW: 0
        SOUP_PICKUP_REWARD: 5
    multi_agent_params:
      bc_schedule:
      - - 0
        - 0
      - - .inf
        - 0
      reward_shaping_factor: 1.0
      reward_shaping_horizon: .inf
      use_phi: true
evaluation_display:
  desc: null
  value: false
evaluation_ep_length:
  desc: null
  value: 400
evaluation_interval:
  desc: null
  value: 1
evaluation_num_games:
  desc: null
  value: 1
evaluation_params:
  desc: null
  value:
    display: false
    ep_length: 400
    num_games: 1
experiment_name:
  desc: null
  value: PPO_cramped_room_True_nw=2_vf=0.000100_es=0.200000_en=0.100000_kl=0.200000
gamma:
  desc: null
  value: 0.99
grad_clip:
  desc: null
  value: 0.1
horizon:
  desc: null
  value: 400
kl_coeff:
  desc: null
  value: 0.2
layout_name:
  desc: null
  value: cramped_room
lmbda:
  desc: null
  value: 0.98
lr:
  desc: null
  value: 5.0e-05
lr_schedule:
  desc: null
  value: null
model_params:
  desc: null
  value:
    CELL_SIZE: 256
    D2RL: false
    NUM_CONV_LAYERS: 3
    NUM_FILTERS: 25
    NUM_HIDDEN_LAYERS: 3
    SIZE_HIDDEN_LAYERS: 64
    use_lstm: false
num_gpus:
  desc: null
  value: 0
num_sgd_iter:
  desc: null
  value: 1
num_training_iters:
  desc: null
  value: 2
num_workers:
  desc: null
  value: 2
params:
  desc: null
  value:
    bc_params:
      bc_config:
        eager: false
        model_dir: /home/anchorwatt/human_aware_rl/human_aware_rl/ppo/bc_runs/default
        stochastic: true
      bc_policy_cls: human_aware_rl.imitation.behavior_cloning_tf2.BehaviorCloningPolicy
    environment_params:
      env_params:
        horizon: 400
      mdp_params:
        layout_name: cramped_room
        rew_shaping_params:
          DISH_DISP_DISTANCE_REW: 0
          DISH_PICKUP_REWARD: 3
          PLACEMENT_IN_POT_REW: 3
          POT_DISTANCE_REW: 0
          SOUP_DISTANCE_REW: 0
          SOUP_PICKUP_REWARD: 5
      multi_agent_params:
        bc_schedule:
        - - 0
          - 0
        - - .inf
          - 0
        reward_shaping_factor: 1.0
        reward_shaping_horizon: .inf
        use_phi: true
    evaluation_params:
      display: false
      ep_length: 400
      num_games: 1
    experiment_name: PPO_cramped_room_True_nw=2_vf=0.000100_es=0.200000_en=0.100000_kl=0.200000
    model_params:
      CELL_SIZE: 256
      D2RL: false
      NUM_CONV_LAYERS: 3
      NUM_FILTERS: 25
      NUM_HIDDEN_LAYERS: 3
      SIZE_HIDDEN_LAYERS: 64
      use_lstm: false
    num_training_iters: 2
    ray_params:
      custom_model_cls: human_aware_rl.ppo.ppo_rllib.RllibPPOModel
      custom_model_id: MyPPOModel
      env_creator: human_aware_rl.ppo.ppo_rllib_client._env_creator
      temp_dir: /tmp/ray_tmp
    results_dir: /home/anchorwatt/ray_results
    save_every: 25
    seeds:
    - 0
    shared_policy: true
    training_params:
      clip_param: 0.05
      eager: false
      entropy_coeff_schedule:
      - - 0
        - 0.2
      - - 300000.0
        - 0.1
      evaluation_interval: 1
      gamma: 0.99
      grad_clip: 0.1
      kl_coeff: 0.2
      lambda: 0.98
      log_level: WARN
      lr: 5.0e-05
      lr_schedule: null
      num_gpus: 0
      num_sgd_iter: 1
      num_workers: 2
      rollout_fragment_length: 400
      seed: null
      sgd_minibatch_size: 800
      train_batch_size: 800
      vf_loss_coeff: 0.0001
      vf_share_layers: true
    verbose: true
params_str:
  desc: null
  value: True_nw=2_vf=0.000100_es=0.200000_en=0.100000_kl=0.200000
ray_params:
  desc: null
  value:
    custom_model_cls: human_aware_rl.ppo.ppo_rllib.RllibPPOModel
    custom_model_id: MyPPOModel
    env_creator: human_aware_rl.ppo.ppo_rllib_client._env_creator
    temp_dir: /tmp/ray_tmp
results_dir:
  desc: null
  value: /home/anchorwatt/ray_results
rew_shaping_params:
  desc: null
  value:
    DISH_DISP_DISTANCE_REW: 0
    DISH_PICKUP_REWARD: 3
    PLACEMENT_IN_POT_REW: 3
    POT_DISTANCE_REW: 0
    SOUP_DISTANCE_REW: 0
    SOUP_PICKUP_REWARD: 5
reward_shaping_factor:
  desc: null
  value: 1.0
reward_shaping_horizon:
  desc: null
  value: .inf
rollout_fragment_length:
  desc: null
  value: 400
save_freq:
  desc: null
  value: 25
seed:
  desc: null
  value: null
seeds:
  desc: null
  value:
  - 0
sgd_minibatch_size:
  desc: null
  value: 800
shared_policy:
  desc: null
  value: true
temp_dir:
  desc: null
  value: /tmp/ray_tmp
train_batch_size:
  desc: null
  value: 800
training_params:
  desc: null
  value:
    clip_param: 0.05
    eager: false
    entropy_coeff_schedule:
    - - 0
      - 0.2
    - - 300000.0
      - 0.1
    evaluation_interval: 1
    gamma: 0.99
    grad_clip: 0.1
    kl_coeff: 0.2
    lambda: 0.98
    log_level: WARN
    lr: 5.0e-05
    lr_schedule: null
    num_gpus: 0
    num_sgd_iter: 1
    num_workers: 2
    rollout_fragment_length: 400
    seed: null
    sgd_minibatch_size: 800
    train_batch_size: 800
    vf_loss_coeff: 0.0001
    vf_share_layers: true
use_lstm:
  desc: null
  value: false
use_phi:
  desc: null
  value: true
verbose:
  desc: null
  value: true
vf_loss_coeff:
  desc: null
  value: 0.0001
vf_share_layers:
  desc: null
  value: true
